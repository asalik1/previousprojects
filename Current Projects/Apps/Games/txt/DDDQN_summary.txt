
This project is an in-depth exploration of advanced reinforcement learning techniques applied to the Overcooked game environment, a challenging cooperative multi-agent setup. The game requires two agents to collaborate in a kitchen setup, performing tasks such as gathering ingredients, cooking, and serving dishes under time constraints. The primary goal set for the agents was to maximize the number of soups delivered within 400 timesteps across five distinct kitchen layouts, aiming for an average of seven soups.

The first approach utilized two independent Deep Q-Networks (DQN) for each agent. DQNs integrate Q-learning with deep neural networks to predict the utility of actions within given states without storing specific state-action values. Instead, a neural network approximates these values, leveraging a replay buffer to minimize sequence bias during learning. This initial model adopted parameters from a previous project involving a single-agent scenario in the Lunar Lander game, emphasizing stable learning rates and long-term strategic planning. However, the complex dynamics and necessary explorative behaviors in Overcooked required adjustments. This led to increased epsilon decay and the number of episodes to enhance state exploration crucial for effective soup delivery strategies. Despite these adjustments, the initial attempts were unsuccessful, prompting a review and adaptation of the strategy.

In the second phase, the focus shifted towards refining reward mechanisms to guide more effective cooperative behaviors and task execution. The modifications included granular reward shaping, where actions such as moving towards the goal with completed dishes were incentivized, whereas less productive actions were penalized. Adjustments were made to encourage essential activities like onion gathering over less crucial ones like dish pickup. Despite improved agent interaction and task execution in some layouts, challenges persisted in others, particularly where agent coordination was crucial but underoptimized.

To address these persistent issues, the architecture was evolved into a Dueling Double Deep Q-Network (DDDQN) enhanced with Prioritized Experience Replay (PER). This new setup featured two significant innovations. First, the double DQN structure, which uses a separate target network for value estimation to avoid overoptimistic predictions inherent in traditional DQNs. This architecture helps in aligning actions more closely with long-term rewards, crucial in cooperative settings. Second, the dueling DQN aspect separates the estimation of state values from the advantages of individual actions, providing a more nuanced understanding of action benefits in given states, particularly beneficial in complex scenarios like Overcooked where understanding the contextual value of specific actions (e.g., carrying a finished soup) is vital.

The PER mechanism was introduced to focus learning on more critical experiences, assigning higher priorities to states closer to successful task completion. This approach aimed to speed up learning by replaying important experiences more frequently, thus addressing the sparsity of rewards in the environment and helping agents learn from significant but rare events.

Despite the sophisticated techniques employed, challenges such as parameter tuning, reward shaping, and the integration of advanced learning architectures like PER required substantial debugging and iterative testing. The learning curve was steep, and the complexity of the environment pushed the limits of the applied methodologies. Nonetheless, the final configurations and strategies led to successful outcomes in most layouts, demonstrating the efficacy of the chosen approaches.

Future work could explore further optimizations, additional reward shaping strategies to enhance cooperative behaviors, and the potential integration of even more advanced algorithms such as Rainbow DQN, which combines multiple improvements over standard DQN architectures for potentially superior performance.
